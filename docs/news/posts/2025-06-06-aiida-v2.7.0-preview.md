---
blogpost: true
category: Blog
tags: release
author: Alexander Goscinski
date: 2025-06-06
---

# AiiDA `v2.7.0` preview

We will soon release `aiida-core` version `v2.7.0`.
There are already release candidates on [pypi](https://pypi.org/project/aiida-core/2.7.0rc1/) and [conda-forge](https://anaconda.org/conda-forge/aiida-core/files?version=2.7.0rc1) as well as [docker image](https://hub.docker.com/layers/aiidateam/aiida-core-dev/aiida-2.7.0pre1/images/sha256-604e375e24d0e626ebdd1f6932378c1f10117732b659337c93b016f305ebe617) for testing purposes.
In this blog post we give an overview of upcoming features of `v2.7.0` and bug fixes.

## Asynchronous SSH connection [#6626](https://github.com/aiidateam/aiida-core/pull/6626)

Previously, when a remote computer was required, the transport plugins blocked further program execution until the communication completed.
This long-standing limitation represented a potential opportunity for performance improvements.

With the introduction of the new asynchronous SSH transport plugin (`core.async_ssh`), multiple communications with a remote machine can now occur concurrently.

### 🚀 When `core.async_ssh` outperforms `core.ssh`

`core.async_ssh` offers significant performance gains in scenarios where the worker is blocked by heavy transfer tasks—such as uploading, downloading, or copying large files.

**Example: Submitting two WorkGraphs/WorkChains with the following logic:**

1. **WorkGraph 1 – Heavy I/O operations**

   - Uploads a 10 MB file
   - Remotely copies a 1 GB file
   - Retrieves a 1 GB file

2. **WorkGraph 2 – Lightweight task**

   - Executes a simple shell command: `touch file`

**Measured time until the second WorkGraph is processed (single worker):**

- **`core.async_ssh`:** **Only 4 seconds!** 🚀🚀🚀🚀 *A dramatic improvement!*
- **`core.ssh`:** **108 seconds** (the second task waits for the first to finish)

### ⚖️ When `core.async_ssh` and `core.ssh` perform similarly

For mixed workloads involving numerous uploads and downloads—a common real-world use case—the performance gains depend on specific conditions.

#### **Large file Transfers (~1 GB):**

`core.async_ssh` typically outperforms due to concurrent upload and download streams.
In favorable network conditions, this can nearly double effective bandwidth.

> *Example: On a network with a baseline of 11.8 MB/s, asynchronous mode approached nearly twice that speed under light load (see graph in PR #6626).*

**Test case:**\
Two WorkGraphs: one uploads 1 GB, the other retrieves 1 GB using `RemoteData`.

- **`core.async_ssh`:** **120 seconds**
- **`core.ssh`:** **204 seconds**

#### **Small file transfers (many small files):**

Here, the overhead of managing asynchronous operations can outweigh the benefits.

**Test case:**\
25 WorkGraphs, each transferring several ~1 MB files.

- **`core.async_ssh`:** **105 seconds**
- **`core.ssh`:** **65 seconds**

## Extended dumping support for profiles and groups [#6723](https://github.com/aiidateam/aiida-core/pull/6723)

In version `v2.6.0`, AiiDA introduced the ability to dump processes from the database into a structured folder format.
Building on this feature, support has now been extended to allow dumping of entire **profiles** and **groups**, enabling users to retrieve all associated data more easily.
This enhancement is part of our broader roadmap to improve AiiDA’s usability, especially for users who may find it challenging to construct the appropriate queries to extract data from the database manually.
The functionality is accessible via the `verdi` CLI:

```bash
verdi profile dump --all # This dumps the whole current profile
verdi profile dump group <PK> # This dumps the whole group
```

Since dumping an entire profile can be a resource-intensive operation, significant effort has been made to provide flexible options for fine-tuning which nodes are included in the dump.
Below is a snippet from the command’s help output:

```bash
Usage: verdi profile dump [OPTIONS] [--]

  Dump all data in an AiiDA profiles storage to disk.

Options:
  -p, --path PATH                 Base path for dump operations that write to
                                  disk.
  -n, --dry-run                   Perform a dry run.
  -o, --overwrite                 Overwrite file/directory when writing to
                                  disk.
  -a, --all                       Include all entries, disregarding all other
                                  filter options and flags.
  -X, --codes CODE...             One or multiple codes identified by their
                                  ID, UUID or label.
  -Y, --computers COMPUTER...     One or multiple computers identified by
                                  their ID, UUID or label.
  -G, --groups GROUP...           One or multiple groups identified by their
                                  ID, UUID or label.
  -u, --user USER                 Email address of the user.
  -p, --past-days PAST_DAYS       Only include entries created in the last
                                  PAST_DAYS number of days.
  --start-date TEXT               Start date for node mtime range selection
                                  for node collection dumping.
  --end-date TEXT                 End date for node mtime range selection for
                                  node collection dumping.
  --filter-by-last-dump-time / --no-filter-by-last-dump-time
                                  Only select nodes whose `mtime` is after the
                                  last dump time.  [default: filter-by-last-
                                  dump-time]
  --only-top-level-calcs / --no-only-top-level-calcs
                                  Dump calculations in their own dedicated
                                  directories, not just as part of the dumped
                                  workflow.  [default: only-top-level-calcs]
  --only-top-level-workflows / --no-only-top-level-workflows
                                  If a top-level workflow calls sub-workflows,
                                  create a designated directory only for the
                                  top-level workflow.  [default: only-top-
                                  level-workflows]
  --delete-missing / --no-delete-missing
                                  If a previously dumped group or node is
                                  deleted from the DB, delete the
                                  corresponding dump directory.  [default:
                                  delete-missing]
  --symlink-calcs / --no-symlink-calcs
                                  Symlink workflow sub-calculations to their
                                  own dedicated directories.  [default: no-
                                  symlink-calcs]
  --organize-by-groups / --no-organize-by-groups
                                  If the collection of nodes to be dumped is
                                  organized in groups, reproduce its
                                  hierarchy.  [default: organize-by-groups]
  --also-ungrouped / --no-also-ungrouped
                                  Dump also data of nodes that are not part of
                                  any group.  [default: no-also-ungrouped]
  --relabel-groups / --no-relabel-groups
                                  Update directories and log entries for the
                                  dumping if groups have been relabeled since
                                  the last dump.  [default: relabel-groups]
  --include-inputs / --exclude-inputs
                                  Include linked input nodes of
                                  `CalculationNode`(s).  [default: include-
                                  inputs]
  --include-outputs / --exclude-outputs
                                  Include linked output nodes of
                                  `CalculationNode`(s).  [default: exclude-
                                  outputs]
  --include-attributes / --exclude-attributes
                                  Include attributes in the
                                  `aiida_node_metadata.yaml` written for every
                                  `ProcessNode`.  [default: include-
                                  attributes]
  --include-extras / --exclude-extras
                                  Include extras in the
                                  `aiida_node_metadata.yaml` written for every
                                  `ProcessNode`.  [default: exclude-extras]
  -f, --flat                      Dump files in a flat directory for every
                                  step of a workflow.
  --dump-unsealed / --no-dump-unsealed
                                  Also allow the dumping of unsealed process
                                  nodes.  [default: no-dump-unsealed]
  -v, --verbosity [notset|debug|info|report|warning|error|critical]
                                  Set the verbosity of the output.
  -h, --help                      Show this message and exit.

```

Another key feature is the incremental mode, which ensures that the dumping process synchronizes with an existing folder structure by gradually adding files. This allows for efficient updates without having to overwrite everything.
The behavior can be adjusted using:

- `--dry-run` (`-n`): to simulate the dump without writing any files.
- `--overwrite` (`-o`): to fully overwrite the target directory if it already exists.

These enhancements aim to make data export from AiiDA more robust, customizable, and user-friendly.

## Stashing [#6746](https://github.com/aiidateam/aiida-core/pull/6746), [#6772](https://github.com/aiidateam/aiida-core/pull/6772)

Now you can bundle your data to a (compressed) tar during the stashing by specifying one of the `stash_mode`s option `"tar"`, `"tar.bz2"`, `"tar.gz"`, `"tar.xz"`.

```python
from aiida.plugins import CalculationFactory
from aiida.engine import run
from aiida.common import StashMode
from aiida.orm import load_computer

inputs = {
    ...,
    'metadata': {
        'computer': load_computer(label="localhost"),
        'options': {
            'resources': {'num_machines': 1},
            'stash': {
                'stash_mode':  StashMode.COMPRESS_TARGZ,
                'target_base': '/scratch/',
                'source_list': ['heavy_data.xyz'], # ['*'] to stash everything
            },
        },
    },
}
# If you use a builder, use
# builder.metadata = {'options': {...}, ...}

run(MyCalculation, **inputs)
```

Historically, stashing was only possible, if it was instructed before running a generic calcjob.
The instruction had to be "attached" to the original calcjob.
However, if a user would realize they need to stash something only after running a CalcJob, this would not be possible.
With `v2.7.0`, we introduce a new CalcJob `StashCalculation` which is able to perform a stashing operation after a calculation has finished.
The usage is very similar, and for consistency and user-friendliness, we keep the instruction as part of the metadata.
The only main input is obviously a source node which is the UUID of the `RemoteData` node of the calculation to be stashed, for example:

```python
from aiida.plugins import CalculationFactory
from aiida.engine import run
from aiida.common import StashMode
from aiida.orm import load_node

StashCalculation = CalculationFactory('core.stash')

calcjob_node = load_node(<CALCJOB_PK>)
inputs = {
    'metadata': {
        'computer': calcjob_node.computer,
        'options': {
            'resources': {'num_machines': 1},
            'stash': {
                'stash_mode':  StashMode.COPY.value,
                'target_base': '/scratch/',
                 'source_list': ['heavy_data.xyz'],
            },
        },
    },
    'source_node': calcjob_node.outputs.remote_folder,
}

result = run(StashCalculation, **inputs)
```

## Forcefully killing process [#6793](https://github.com/aiidateam/aiida-core/pull/6793)

Prior to version `v2.7.0`, the `verdi process kill` command could hang if a connection to the remote computer could not be established.
A new `--force` option has been introduced to terminate a process without waiting for a response from the remote machine.\
**Note:** Using `--force` may result in orphaned jobs on the remote system if the remote job cancellation fails.

```bash
verdi process kill --force <PROCESS_ID>
```

We also now cancel the old killing action if it is resend by the user.
This allows the user to adapt the EBM parameters in the verdi config and then resend the kill command with the new parameters.

```bash
verdi process kill --timeout 5 <PROCESS_ID>
verdi config set transport.task_maximum_attempts 1
verdi config set transport.task_retry_initial_interval 5
verdi daemon restart
verdi process kill <PROCESS_ID>
```

Furthermore, the `timeout` and `wait` were not correctly behaving.
We fixed this and merged the functionality of `wait` into `timeout`.
By passing `--timeout 0` it replicates the `--no-wait` functionality, the command does not block until the action has finished, and by passing `--timeout inf` (default option) it replicates `--wait` without a timeout, the command blocks until a response.
For more information see issue #6524.

## Serialization of ORM nodes [#6723](https://github.com/aiidateam/aiida-core/pull/6723)

AiiDA's Python API provides an object relational mapper (ORM) that abstracts the various entities that can be stored inside the provenance graph and the relationships between them.
In most use cases, users use this ORM directly in Python to construct new instances of entities and retrieve existing ones, in order to get access to their data and manipulate it.
A current shortcoming of the ORM is that it is not possible to programmatically introspect the schema of each entity: that is to say, what data each entity stores.
This makes it difficult for external applications to provide interfaces to create and or retrieve entity instances.
It also makes it difficult to take the data outside of the Python environment since the data would have to be serialized.
However, without a well-defined schema, doing this without an ad-hoc solution is practically impossible.

With the implementation of a Model for each Entity we now allow an external application to programmatically determine the schema of all entities of AiiDA's ORM and automatically (de)serialize entity instances to and from other data formats, e.g., JSON.
An example how this is done for an integer node.

```python
node = Int(5) # Can be any ORM node
serialized_node = node.serialize()
print(serialized_node)
# Out: {'pk': None, 'uuid': '485c2ec8-441d-484d-b7d9-374a3cdd98ae', 'node_type': 'data.core.int.Int.', 'process_type': None, 'repository_metadata': {}, 'ctime': datetime.datetime(2025, 5, 2, 10, 20, 41, 275443, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST')), 'mtime': None, 'label': '', 'description': '', 'attributes': {'value': 5}, 'extras': {}, 'computer': None, 'user': 1, 'repository_content': {}, 'source': None, 'value': 5}
uuid: 77e9c19a-5ecb-40cf-8238-ea5c55fbb83f (unstored) value: 5
node_deserialized = Int.from_serialized(**serialized_node)
print(node_deserialized)
# Out: uuid: 77e9c19a-5ecb-40cf-8238-ea5c55fbb83f (unstored) value: 5
```

For an extensive overview of the implications see [AEP 010](https://github.com/aiidateam/AEP/blob/983a645c9285ba65c7cf07fe6064c23e7e994c06/010_orm_schema/readme.md)

## Miscellaneous

- aiida-core is compatible with Python 3.13 [#6600](https://github.com/aiidateam/aiida-core/pull/6600)
- Improved windows support [#6715](https://github.com/aiidateam/aiida-core/pull/6715)
- `RemoteData` extended by member function`get_size_on_disk` [#6584](https://github.com/aiidateam/aiida-core/pull/6584)
- `SinglefileData` extended by constructor `from_bytes` [#6653](https://github.com/aiidateam/aiida-core/pull/6653)
- Allow zero memory specification for SLURM [#6605](https://github.com/aiidateam/aiida-core/pull/6605)
- Add filters to `verdi group delete` [#6556](https://github.com/aiidateam/aiida-core/pull/6556)
- `verdi storage maintain` shows a progress bar [#6562](https://github.com/aiidateam/aiida-core/pull/6562)
- New transport endpoint `compress` & `extract` [#6743](https://github.com/aiidateam/aiida-core/pull/6743)
- Implementation of missing sqlite endpoints:
  - `get_creation_statistics` [#6763](https://github.com/aiidateam/aiida-core/pull/6763)
  - `contains` [#6619](https://github.com/aiidateam/aiida-core/pull/6619)
  - `has_key` [#6606](https://github.com/aiidateam/aiida-core/pull/6606)
